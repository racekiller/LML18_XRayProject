{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Machine Learning 2018\n",
    "## Universidad del Rosario\n",
    "\n",
    "##### Integrantes\n",
    "##### German\n",
    "##### Hugo\n",
    "##### Arturo\n",
    "##### Jimmy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jvivas/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jul 27 16:46:10 2018\n",
    "\n",
    "@author: felipe\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import shutil\n",
    "import imgaug as aug\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mimg\n",
    "import imgaug.augmenters as iaa\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras.models import Sequential, Model\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\n",
    "from keras.layers import GlobalMaxPooling2D, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Set the seed for hash based operations in python\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Set the numpy seed\n",
    "np.random.seed(111)\n",
    "\n",
    "# Disable multi-threading in tensorflow ops\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "# Set the random seed in tensorflow at graph level\n",
    "tf.set_random_seed(111)\n",
    "\n",
    "# Define a tensorflow session with above session configs\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "\n",
    "# Set the session in keras\n",
    "K.set_session(sess)\n",
    "\n",
    "# Make the augmentation sequence deterministic\n",
    "aug.seed(111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to the data directory\n",
    "data_dir = Path('/Users/jvivas/Documents/Kaggle/XRay Pneumonia/data/chest_xray')\n",
    "\n",
    "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
    "train_dir = data_dir / 'train'\n",
    "\n",
    "# Path to validation directory\n",
    "val_dir = data_dir / 'val'\n",
    "\n",
    "# Path to test directory\n",
    "test_dir = data_dir / 'test'\n",
    "\n",
    "normal_casos_dir = train_dir / 'NORMAL'\n",
    "pneumonia_casos_dir = train_dir / 'PNEUMONIA'\n",
    "\n",
    "# put the images in a list\n",
    "normal_casos_lista = normal_casos_dir.glob('*.jpeg')\n",
    "pneumonia_casos_lista = pneumonia_casos_dir.glob('*.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = []\n",
    "\n",
    "img_size = [225, 225]\n",
    "\n",
    "# agregamos la label a cada foto\n",
    "for img in normal_casos_lista:\n",
    "    train_data1.append((img,0))\n",
    "\n",
    "for img in pneumonia_casos_lista:\n",
    "    train_data1.append((img,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffeling Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "s = np.arange(len(train_data1))\n",
    "np.random.shuffle(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Images Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [0]*len(train_data1)\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = train_data1[s[i]]\n",
    "\n",
    "\n",
    "train_img_data =[]\n",
    "train_labels = []\n",
    "for i in range(len(train_data)):\n",
    "    img = cv2.imread(str(train_data[i][0]))\n",
    "    if img.shape[2]==3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (img_size[0], img_size[1]))\n",
    "    post_img= cv2.equalizeHist(img)\n",
    "    label = train_data[i][1]\n",
    "    #plt.imshow(post_img, cmap='gray')\n",
    "    post_img = post_img.reshape(225,225,1)\n",
    "    train_img_data.append(post_img)\n",
    "    train_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_img_data) / 255\n",
    "Y_train = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, ker_1 = 10,  ker_2 =20 , drop = 0.05):\n",
    "        # initialize the model\n",
    "        ker_1_size=(7,7)\n",
    "        ker_2_size=(7,7)\n",
    "\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width,1)\n",
    "\n",
    "        # first set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(ker_1, ker_1_size, input_shape=inputShape, padding=\"same\", kernel_initializer = 'normal'))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # second set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(ker_2, ker_2_size, padding=\"same\", kernel_initializer = 'normal'))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),)\n",
    "        model.add(Dropout(drop))\n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        # softmax classifier\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN Model\n",
    "model = LeNet.build(img_size[0], img_size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model \n",
    "chkpt = ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=True)\n",
    "es = EarlyStopping(patience=5)\n",
    "model.compile(loss = 'binary_crossentropy' \n",
    "              , optimizer = 'adagrad' \n",
    "              , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4172 samples, validate on 1044 samples\n",
      "Epoch 1/20\n",
      "4172/4172 [==============================] - 800s 192ms/step - loss: 4.1690 - acc: 0.7385 - val_loss: 3.8176 - val_acc: 0.7605\n",
      "Epoch 2/20\n",
      "4172/4172 [==============================] - 600s 144ms/step - loss: 4.1690 - acc: 0.7385 - val_loss: 3.8176 - val_acc: 0.7605\n",
      "Epoch 3/20\n",
      "4172/4172 [==============================] - 599s 144ms/step - loss: 4.1690 - acc: 0.7385 - val_loss: 3.8176 - val_acc: 0.7605\n",
      "Epoch 4/20\n",
      "4172/4172 [==============================] - 597s 143ms/step - loss: 4.1690 - acc: 0.7385 - val_loss: 3.8176 - val_acc: 0.7605\n",
      "Epoch 5/20\n",
      "4172/4172 [==============================] - 598s 143ms/step - loss: 4.1690 - acc: 0.7385 - val_loss: 3.8176 - val_acc: 0.7605\n",
      "Epoch 6/20\n",
      "4172/4172 [==============================] - 598s 143ms/step - loss: 4.1690 - acc: 0.7385 - val_loss: 3.8176 - val_acc: 0.7605\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size = 16, epochs = 20,\n",
    "                    validation_split = 0.2, verbose =1\n",
    "                    ,callbacks=[es, chkpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "model.load_weights('/Users/jvivas/Documents/Kaggle/XRay Pneumonia/best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_casos_dir = test_dir / 'NORMAL'\n",
    "pneumonia_casos_dir = test_dir / 'PNEUMONIA'\n",
    "\n",
    "# put the images in a list\n",
    "normal_casos_lista = normal_casos_dir.glob('*.jpeg')\n",
    "pneumonia_casos_lista = pneumonia_casos_dir.glob('*.jpeg')\n",
    "\n",
    "test_data = []\n",
    "\n",
    "# agregamos la label a cada foto\n",
    "for img in normal_casos_lista:\n",
    "    test_data.append((img,0))\n",
    "\n",
    "for img in pneumonia_casos_lista:\n",
    "    test_data.append((img,1))\n",
    "\n",
    "test_img_data =[]\n",
    "test_labels = []\n",
    "for i in range(len(test_data)):\n",
    "    img = cv2.imread(str(test_data[i][0]))\n",
    "    if img.shape[2]==3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (img_size[0], img_size[1]))\n",
    "    post_img= cv2.equalizeHist(img)\n",
    "    label = train_data[i][1]\n",
    "    #plt.imshow(post_img, cmap='gray')\n",
    "    post_img = post_img.reshape(225,225,1)\n",
    "    test_img_data.append(post_img)\n",
    "    test_labels.append(label)\n",
    "\n",
    "# Normalization\n",
    "X_test = np.array(test_img_data) / 255\n",
    "Y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "65afc715-9f5b-4a3a-b2a9-053bd271ba94",
    "_uuid": "8032be06a247d736a041f83f0f78d74ee0310a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 [==============================] - 35s 56ms/step\n",
      "Loss on test set:  4.266631966982134\n",
      "Accuracy on test set:  0.7323717948717948\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test dataset\n",
    "test_loss, test_score = model.evaluate(X_test, Y_test, batch_size=16)\n",
    "print(\"Loss on test set: \", test_loss)\n",
    "print(\"Accuracy on test set: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bfc1e8956055abdb5d5bb3399509864aa635ba6"
   },
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "preds = model.predict(Y_test, batch_size=16)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "# Original labels\n",
    "orig_test_labels = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "print(orig_test_labels.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "761b7db9-0529-4fb5-a049-114d164cdd67",
    "_uuid": "5384c318ee60802ce2ec73aea2632b75586c34bb",
    "collapsed": true
   },
   "source": [
    "When a particular problem includes an imbalanced dataset, then accuracy isn't a good  metric to look for. For example, if your dataset contains 95 negatives and 5 positives, having a model with 95% accuracy doesn't make sense at all. The classifier might label every example as negative and still achieve 95% accuracy. Hence,  we need to look for alternative metrics. **Precision** and **Recall** are really good metrics for such kind of problems. \n",
    "\n",
    "We will get the confusion matrix from our predictions and see what is the recall and precision of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a0944856f2d0666cf7c4fd90ad3e690ed17b2e7"
   },
   "outputs": [],
   "source": [
    "# Get the confusion matrix\n",
    "cm  = confusion_matrix(orig_test_labels, preds)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, alpha=0.7,cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7429c904fed42eacc83dc385a7c83b1c47b64183"
   },
   "outputs": [],
   "source": [
    "# Calculate Precision and Recall\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "\n",
    "print(\"Recall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12900d098502134c4a51527cdf5a2c1203ecaa36",
    "collapsed": true
   },
   "source": [
    "Nice!!! So, our model has a 98% recall. In such problems, a good recall value is expected. But if you notice, the precision is only 80%. This is one thing to notice. Precision and Recall follows a trade-off, and you need to find a point where your recall, as well as your precision, is more than good but both can't increase simultaneously. \n",
    "\n",
    "That's it folks!! I hope you enjoyed this kernel. Happy Kaggling!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcda8ddec0c440e1de216905807e6f63d741743f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
